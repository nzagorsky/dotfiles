#!/usr/bin/env python3

"""
Ingests CUR/DBR from current folder to PostgreSQL and drops tags

Usage:

    cur2psql /cur/path
    cur2psql  # in CUR folder
    cur2psql dbr.csv.zip
    AWS_PROFILE=profilename cur2psql s3://cur/path
"""

import csv
import glob
import gzip
import json
import logging
import os
import sys
import time
from io import TextIOWrapper
from queue import Empty
from queue import Queue
from threading import Thread
from typing import Any
from typing import Dict
from typing import Iterator
from typing import List
from typing import Tuple
from zipfile import ZipFile

import boto3
import smart_open
from trino.dbapi import connect
from trino.exceptions import TrinoExternalError

logging.basicConfig(level=logging.INFO)

# Connection config
connection = connect(
    host="toltenos-ubuntu.local",
    port="8080",
    user="admin",
    catalog="iceberg",
    schema="billing",
)

# Ingestion config
BATCH_SIZE = 500
QUEUE_SIZE = BATCH_SIZE * 6
WORKER_COUNT = 8
LIMIT_TO = 1000000000000000

queue = Queue(maxsize=QUEUE_SIZE)
rows_queue = Queue(maxsize=QUEUE_SIZE)

# connection = psycopg2.connect(
#     f"dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} host={DB_HOST} port={DB_PORT}"
# )

logging.info("Database connection established successfully.")


class IterationState:
    finished_iteration = False


def get_boto3_session() -> boto3.Session:
    if "AWS_PROFILE" in os.environ:
        session = boto3.Session(profile_name=os.environ["AWS_PROFILE"])
    else:
        session = boto3.Session()
    return session


def process_input_parameters() -> Tuple[bool, bool, str]:
    is_cur = False
    is_remote = False

    if len(sys.argv) > 1:
        filepath = sys.argv[1]

        if "s3://" in filepath:
            is_cur = not filepath.endswith(".csv.zip")
            is_remote = True
            if is_cur and not filepath.endswith("/"):
                filepath += "/"
            return is_cur, is_remote, filepath

        filepath = os.path.join(os.getcwd(), sys.argv[1])
    else:
        is_cur = True
        filepath = os.getcwd()

    return is_cur, is_remote, filepath


def get_table_name(is_cur: bool, is_remote: bool, filepath: str) -> str:
    if is_cur:
        if is_remote:
            cur_date = filepath.split("/")[5].split("-")[0]
            report_name = filepath.split("/")[4]
            table_identifier = "_".join([report_name, cur_date])
        else:
            table_identifier = filepath.split("/")[-1]

        table_name = f"cur_csv_{table_identifier}".replace("-", "_")
    else:
        filename = filepath.split("/")[-1].replace("-", "_").strip(".csv.zip")
        filename_elements = filename.split("_")
        table_name = f"dbr_csv_{filename_elements[0]}_"  # Account number
        table_name += "_".join(filename_elements[-2:])  # Date

    logging.info(f"Using {table_name} table")
    return table_name


def create_table(table_name: str, columns: List[str]) -> None:
    cursor = connection.cursor()
    try:
        cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
        cursor.fetchall()

        field_definitions = []
        for column in columns:
            if "blendedcost" in column.lower():
                col_type = "DOUBLE"
            else:
                col_type = "VARCHAR(512)"

            field_definitions.append(f"{column} {col_type}")

        table_fields = ",\n".join(field_definitions)

        schema_script = f"CREATE TABLE {table_name} ( {table_fields} )"
        cursor.execute(schema_script)
        cursor.fetchall()
    finally:
        cursor.close()

    connection.commit()


def ingest_items(table_name: str, items: List[Dict[str, Any]]):
    if not items:
        return

    keys = items[0].keys()
    insertion_statement = f"""
    INSERT INTO {table_name} ({','.join(keys)}) VALUES
    """
    for item in items:
        values = ",".join(
            [
                f"'{value}'" if "blendedcost" not in key.lower() else f"{value}"
                for key, value in item.items()
            ]
        )
        insertion_statement += f"\n ({values}),"

    insertion_statement = insertion_statement.rstrip(",")

    cursor = connection.cursor()

    while True:
        try:
            cursor.execute(insertion_statement)
            cursor.fetchall()
            break
        except TrinoExternalError:
            print("error TrinoExternalError")
            time.sleep(0.1)

    cursor.close()
    connection.commit()


def worker(table_name: str, state: IterationState) -> None:
    to_ingest = []
    print("Starting ingest worker. Table:", table_name)

    while True:
        try:
            _element = queue.get(block=True, timeout=10)

            to_ingest.append(_element)
            if len(to_ingest) == BATCH_SIZE:
                ingest_items(table_name=table_name, items=to_ingest)
                to_ingest = []

        except Empty:
            if state.finished_iteration:
                ingest_items(table_name=table_name, items=to_ingest)
                logging.info("Exiting PSQL ingestion worker, iteration is finished.")
                break
            else:
                time.sleep(0.1)


def get_report_keys(filepath: str) -> Iterator[str]:
    session = get_boto3_session()
    client = session.client("s3")
    # bucket_name = filepath.split("/")[2]
    # prefix = "/".join(filepath.split("/")[3:])
    manifest_name = filepath.split("/")[4]
    manifest_path = f"{filepath}{manifest_name}-Manifest.json"

    with smart_open.open(manifest_path, transport_params={"client": client}) as f:  # type: ignore
        manifest_data = json.load(f)

    for key in manifest_data["reportKeys"]:
        yield key


def s3_rows_fetcher(filepath: str):
    session = get_boto3_session()
    client = session.client("s3")
    bucket_name = filepath.split("/")[2]

    for report_key in get_report_keys(filepath=filepath):
        with smart_open.open(f"s3://{bucket_name}/{report_key}", transport_params=dict(client=client)) as report_handler:  # type: ignore
            reader = csv.DictReader(report_handler)
            for row in reader:
                rows_queue.put(row)


def get_rows(is_cur: bool, is_remote: bool, filepath: str) -> Iterator[Dict[str, Any]]:
    if is_remote:
        if is_cur:
            s3_worker = Thread(target=s3_rows_fetcher, args=(filepath,))
            s3_worker.start()
            while True:
                try:
                    yield rows_queue.get(block=True, timeout=1)
                except Empty:
                    logging.debug(f"Empty rows buffer for 1 sec")
                    if not s3_worker.is_alive():
                        break
        else:
            logging.error("Remote DBRs are not supported")

    else:
        if is_cur:
            cur_found = glob.glob(f"{filepath}/*.csv.gz")
            manifest_found = glob.glob(f"{filepath}/*-Manifest.json")

            if not cur_found and manifest_found and len(manifest_found) == 1:
                with open(manifest_found[0]) as manifest_f:
                    manifest_data = json.load(manifest_f)
                    assembly_id = manifest_data["assemblyId"]
                    cur_found = glob.glob(f"{filepath}/{assembly_id}/*.csv.gz")

            if not cur_found:
                raise ValueError(f"Cannot find CUR in {filepath}")

            for filename in cur_found:
                logging.info(f"Reading {filename}")
                with gzip.GzipFile(filename) as zf:
                    reader = csv.DictReader(TextIOWrapper(zf, "utf-8"))  # type: ignore
                    for row in reader:
                        yield row
        else:
            with ZipFile(filepath) as zf:
                with zf.open(zf.filelist[0].filename) as zipped_csv:
                    reader = csv.DictReader(TextIOWrapper(zipped_csv, "utf-8"))
                    for row in reader:
                        yield row


def main() -> None:
    row_counter = 0
    table_initialized = False

    is_cur, is_remote, filepath = process_input_parameters()
    table_name = get_table_name(is_cur=is_cur, is_remote=is_remote, filepath=filepath)

    # Start workers
    state = IterationState()
    ingest_workers = [
        Thread(target=worker, args=(table_name, state)) for _ in range(WORKER_COUNT)
    ]
    _ = [t.start() for t in ingest_workers]

    previous_time = time.time()
    for row in get_rows(is_cur=is_cur, is_remote=is_remote, filepath=filepath):
        if is_cur:
            row = {
                k.replace("/", "_"): v
                for k, v in row.items()
                if not k.startswith("resourceTags")
            }
        else:
            row = {k: v for k, v in row.items() if ":" not in k}

        if not table_initialized:
            create_table(table_name=table_name, columns=list(row.keys()))
            table_initialized = True

        queue.put(row)
        row_counter += 1

        if not row_counter % 100000:
            logging.info(
                f"Inserted {row_counter} Took {time.time()-previous_time:.2f} s"
            )
            previous_time = time.time()

        if row_counter == LIMIT_TO:
            break

    state.finished_iteration = True
    logging.info("Joining threads")
    _ = [t.join() for t in ingest_workers]
    thread_results = [not t.is_alive() for t in ingest_workers]
    logging.info(f"Joined threads: {all(thread_results)}")
    logging.info(f"Total ingested: {row_counter}")


if __name__ == "__main__":
    main()
